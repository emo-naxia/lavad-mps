import sys
import os
import importlib.util
import multiprocessing
import time

# é¡¹ç›®ä¸»ç›®å½•
PROJECT_ROOT = "/Users/emo/lavad/lavad-main"

# 1ï¸âƒ£ å¼ºåˆ¶æŠŠé¡¹ç›®çš„ libs è·¯å¾„æ”¾åˆ° sys.path æœ€å‰é¢
PROJECT_LIBS_PATH = os.path.join(PROJECT_ROOT, "libs")
if PROJECT_LIBS_PATH not in sys.path:
    sys.path.insert(0, PROJECT_LIBS_PATH)

# 2ï¸âƒ£ æ‰‹åŠ¨åŠ è½½ llama æ¨¡å—
LLAMA_INIT = os.path.join(PROJECT_LIBS_PATH, "llama", "llama", "__init__.py")
spec = importlib.util.spec_from_file_location("llama", LLAMA_INIT)
llama = importlib.util.module_from_spec(spec)
sys.modules["llama"] = llama
spec.loader.exec_module(llama)

print(f"âœ… æˆåŠŸåŠ è½½ llama æ¨¡å—: {llama.__file__}")

# æ£€æµ‹è®¾å¤‡
try:
    from llama.generation import DEVICE
    print(f"ğŸš€ å½“å‰ä½¿ç”¨çš„æ¨ç†è®¾å¤‡: {DEVICE}")
except ImportError:
    print("âš ï¸ æ— æ³•æ£€æµ‹ DEVICEï¼Œå¯èƒ½æœªæ­£ç¡®ä¿®æ”¹ generation.py")

# 3ï¸âƒ£ ä» llama å¯¼å…¥ simple_infer
from llama import simple_infer

# 4ï¸âƒ£ æ¨¡å‹è·¯å¾„
model_path = "/Users/emo/models/qwen1_5-7b-chat-q4_k_m.gguf"
if not os.path.exists(model_path):
    raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")

# çº¿ç¨‹ä¼˜åŒ–ï¼ˆç•™ä¸€ç‚¹ç»™ç³»ç»Ÿï¼‰
threads = max(1, multiprocessing.cpu_count() - 2)

# 5ï¸âƒ£ æ¨ç†æµ‹è¯•ï¼ˆå¸¦è®¡æ—¶ï¼‰
prompt = "ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½ã€‚"
start_time = time.time()

output = simple_infer(
    prompt=prompt,
    model_path=model_path,
    max_tokens=50,
    temperature=0.7,
    n_ctx=4096,
    n_threads=threads,
    n_batch=512
)

end_time = time.time()
elapsed = end_time - start_time
tokens_generated = len(output.split())  # ç®€å•ç”¨è¯æ•°ä¼°ç®—
speed = tokens_generated / elapsed

print("ğŸ’¡ æ¨¡å‹è¾“å‡º:", output)
print(f"â±ï¸ è€—æ—¶: {elapsed:.2f} ç§’ | âš¡ é€Ÿåº¦: {speed:.2f} tokens/s (ä¼°ç®—)")